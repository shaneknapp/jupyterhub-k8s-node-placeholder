replicaCount: 1

image:
  repository: us-central1-docker.pkg.dev/cal-icor-hubs/core/node-placeholder-scaler
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "20240731-224556.git.288.h5ae00c2e"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

resources: {}

nodeSelector: {}


affinity: {}

rbac:
  create: true

priorityClass:
  create: true


tolerations:

# The URL of the public calendar to use for the node placeholder
calendarUrl: 
  https://calendar.google.com/calendar/ical/c_35d90f50598c1472a4154c538bb49a21eabd8be93831d7de345d53fea8e19390%40group.calendar.google.com/public/basic.ics

grafana:
  tags:
    - node-placeholder-scale
  url: https://grafana.datahub.berkeley.edu


# Calculate size of node placeholder by:
# 1. Select a node in the node pool you want to configure
#    export NODE=<node-name>
# 2. Get total amount of memory allocatable to pods on this node
#    kubectl get node $NODE -o jsonpath='{.status.allocatable.memory}'
# 3. Get total amount of memory used by non-user pods on the node.
#    kubectl get -A pod -l 'component!=user-placeholder' --field-selector spec.nodeName=$NODE -o jsonpath='{range .items[*].spec.containers[*]}{.name}{"\t"}{.resources.requests.memory}{"\n"}{end}' | egrep -v 'pause|notebook' | sort
#    This will return memory units you'll need to sum yourself
#
# The node placeholder pod should be 'big' enough that it needs to be kicked out to get even a single
# user pod on the node - but not so big that it can't run on a node where other system pods are running!
#
# So we should make the memory be output of (2) - output of (3), with maybe another 256Mi wiggle room
#
# Example using a GCP n2-highmem-8 node with 64G of RAM allocatable:
# $ NODE=<node name>
# $ kubectl get node ${NODE} -o jsonpath='{.status.allocatable.memory}' # convert to bytes
# 60055600Ki%
# $ kubectl get -A pod -l 'component!=user-placeholder' \
#   --field-selector spec.nodeName=${NODE} \
#   -o jsonpath='{range .items[*].spec.containers[*]}{.name}{"\t"}{.resources.requests.memory}{"\n"}{end}' | \
#   egrep -v 'pause|notebook' # convert all values to bytes for easier mathing
# calico-node
# fluentbit       100Mi
# fluentbit-gke   100Mi
# gke-metrics-agent       60Mi
# ip-masq-agent   16Mi
# kube-proxy
# prometheus-node-exporter
# $ # subtract the sum of the second command's values from the first value, then subtract another 277872640 bytes for wiggle room
# $ # in this case:  (60055600Ki - (100Mi + 100Mi + 60Mi + 16Mi)) - 256Mi
# $ # (61496934400 - (104857600 + 104857600 + 16777216 + 62914560)) - 277872640 == 60929654784
#
# FIXME: LET US MAKE THIS INTO A SCRIPT!
nodePools:
  jupyter:
    nodeSelector:
      hub.jupyter.org/pool-name: jupyter-pool
    resources:
      requests:
        # Some value slightly lower than allocatable RAM on the nodepool
        # This is an example using a GCP n2-highmem-8 node with 64G of RAM allocatable
        memory: 60929654784
    replicas: 0
